This master thesis is divided into four major parts. In the first part, which consists of chapters \ref{chap:prerequisites} and \ref{chap:relatedWork}, the reader is introduced to the generic reinforcement learning framework, the Automatic Identification System (AIS) and the recent developments of reinforcement- and imitation learning in general. In addition to that, a big overview is shown in the related work chapter which also presents deviating methods and approaches aside from reinforcement learning.
\par
After finishing the first part and chapter \ref{chap:relatedWork}, the next part deals with the functioning, processes and general assumptions of the algorithms and methods used in this thesis on a very theoretical level. Chapter \ref{chap:DDPG} dives deep into the inner workings of a popular reinforcement learning algorithm based on the actor-critic architecture, DDPG, whereas chapter \ref{chap:imitation} presents an entirely different approach to the overall task as in directly mimicking the expert's behavior and argues why imitation learning might be a better choice.
\par
The third part and chapter \ref{chap:synthetic} constructs a synthetic, simplified version of the real-world environment in order to investigate the performances of different experimental setups with different state spaces, algorithms, and hyperparameters. Findings and assumptions are then used in the next part, the building of the actual prediction system based on historical AIS data. In this chapter \ref{chap:realworld}, the whole process of extracting vessel trajectories, defining different training sets, running experiments, evaluation as well as failure tracing and the proposal of possible improvements take place. In the end, conclusions are drawn in chapter \ref{chap:conclusions}.