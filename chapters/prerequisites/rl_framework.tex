In a reinforcement learning setup an agent is interacting with an environment $E$ at discrete time steps. At every time step $t$ the agent observes the current state of the environment $s_t$ and selects an action $a_t$. Feedback is returned by the environment in the form of a scalar reward $r_{t+1}$ and the resulting state $s_{t+1}$ based on the internal dynamics of the environment. We will use the definitions of \cite{Sutton1998} throughout this chapter, especially the definition of the point in time the reward is given back. In contrast to other publications like those from \cite{zare2021continuous}, \cite{wiering2012reinforcement} and \cite{lillicrap2019continuous}, \cite{Sutton1998} do not bind the reward signal to the same time step as the current state-action pair ($s_t$, $a_t$) but rather the next one, to indicate that the reward and the next state are calculated at the same time $t+1$.
This interaction can thus be illustrated in the following way, where the uppercase symbols $A$, $S$, and $R$ are used to indicate random variables. Their concrete instances of states, actions, or rewards are specified by the corresponding their lowercase symbols (e.g., $s_t$, $s_{t+1}$, or $a_t$):
\begin{figure}[H]
    \centering
    \hspace*{1cm}%
    \begin{tikzpicture}[]
\node (A) at (3,3) [rectangle, rounded corners, draw, minimum height=1cm,minimum width=3cm] {Environment};
\node (B) at (3,0) [rectangle, rounded corners, draw, minimum height=1cm,minimum width=3cm] {Agent};

\node (C) at (0.7, 3) [rectangle, thick, dashed, inner sep=0pt, minimum height=1cm, minimum width=0cm, draw] {};

\draw[->, black, very thick] ($(B.east)$) to[out=0,in=0, distance=2.5cm] ($(A.east)$);

\draw[->, black, very thick] ($(A.west)-(0.1,0)$)  to[out=3,in=7] ($(C.east) -(0,0.02)$);

\draw[->, black, very thick] ($(C.east) - (0,0.02)$) to[out=193,in=180, distance=2.4cm] ($(B.west)$);

\node[text width=3cm] at (1.6 , 4) 
    {$R_{t+1} S_{t+1}$};
    
\node[text width=3cm] at (-1.6 , 0.8) 
    {Observation \\ $S_t$};

\node[text width=3cm] at (-1.6, 2.2) 
    {Reward \\ $R_t$};
    
    
\node[text width=3cm] at (8.3, 1.5) 
    {Action \\ $A_t$};
    
\end{tikzpicture}
    \caption{Agent-environment interaction.}
    \label{fig:rlEnvironment}
\end{figure}



To be precise, we formalize the environment as a Markov decision process (MDP) with state space $\mathcal{S}$, continuous action space $\mathcal{A}$ , the transition dynamics $p(s_{t+1} \mid s_t, a_t)$ and reward function $r(s_t, a_t)$. The reward function is hand-crafted and indicates whether the selected action in a certain state is considered "good" or "bad". The action selection process or rather the \textit{behavior} of the agent is defined by a policy $\pi$, which is essentially a mapping from states to probabilities of actions being chosen $\pi: S \rightarrow P(A)$. But as we will exclusively calculate deterministic policies in this thesis, we define $\pi: S \rightarrow A$.
\newpage
One of the major challenges arises from the nature of an MDP being a sequential decision problem. An action that seems to be mediocre in the present may result in a trajectory that produces better rewards in the long run and vice versa. It is therefore necessary to take future rewards into account. The discounted sum of future rewards is called the \textit{return} with a discounting factor $\gamma \in (0,1]$  \cite[p.55]{Sutton1998}:

\begin{equation}\label{eq:discountedReturn}
    G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots  = \sum_{k=0}^\infty{\gamma^k R_{t+k+1}}
\end{equation}

By reshaping the above equation, we can discover the relationship between consecutive returns \cite[p.55]{Sutton1998}:
\begin{equation}\label{eq:successiveReturn}
    \begin{aligned}
    G_t &= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots \\
    &= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots)  \\
   & = R_{t+1} + \gamma G_{t+1}
    \end{aligned}
\end{equation}

The overall objective of a reinforcement learning agent is therefore to find an optimal policy that maximizes the return for a given starting state $s_0$. In order to do so, reinforcement learning algorithms estimate the expected return for any given state or state-action pair, which is also called the \textit{value} or $Q$-value. The recursive relationship revealed in equation \ref{eq:successiveReturn} naturally applies to $Q$-values as well, which is formally described by the \textit{Bellman equation} (for discrete policies):

\begin{equation}
    Q^\pi(s_t, a_t) = \EX_\pi[r(s_t, a_t) + \gamma Q^\pi(s_{t+1}, \pi(s_{t+1})]
\end{equation}

Unfortunately, it is impossible to solve this equation if the model of the environment is unknown. Even if a perfect model is present, methods like \textit{Dynamic Programming} have limited utility due to their great computational expense \cite[p.~73]{Sutton1998}. Therefore, reinforcement learning algorithms take an iterative approach by interacting with the environment, sampling feedback and consecutively updating estimates of the actual returns. The closer those estimates are to the real values, the better the derived policy because the greediest policy will just select the action with the highest $Q$-value for every state.
\par
Tabular methods, as the name suggests,  keep all state-action pairs and their corresponding $Q$-value in big tables, updating them if needed. In contrast, deep reinforcement learning techniques utilize powerful function approximators like neuronal networks for the purpose of replacing the memory-limited tables and handling huge state and action spaces.