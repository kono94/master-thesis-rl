This subchapter gives an overview of the latest developments and achievements in the research field of deep reinforcement learning, starting with the key paper that is responsible for the re-ignition and the explosion of interest in reinforcement learning, the work from \cite{mnih2013playing, mnih2015human}.
\par 
\cite{mnih2013playing} propose an algorithm called Deep Q-Network
(DQN) that combines Q-Learning with a neural network, which is used to estimate the Q-values instead of using a look-up table. One of the reasons the paper got so much attention is due to the fact that the algorithm learned to play seven Atari games on human like level just based on the pure pixel input without changing the architecture (p.~7-8). The main difference between the presented method by \cite{mnih2013playing} in contrast to past approaches of using neural networks in conjunction with reinforcement learning, is the introduction of an experience replay and a target network to ensure stability during the training process. We go into detail on those components later in this thesis.
\par
Four years after the initial publication of DQN, \cite{hessel2018rainbow} combine the suggested improvements to DQN by the research community to propose an algorithm called \anf{Rainbow}. Although DQN and Rainbow can be seen as milestone methods, they are still only capable of handling discrete action spaces. Instead of estimating Q-values, policy gradient methods represent the policy by a parametric probability distribution that either stochastically or deterministically selects an action. Those methods are used to be able to operate in the continuous action space, as this is often a mandatory requirement for example in robotic control. Examples for algorithms that are labeled as the stochastic case are Proximal Policy Optimization (PPO) \cite[]{schulman2017proximal} and Trust Region Policy Optimization (TRPO) \cite[]{schulman2017trust}.
\par 
A deterministic policy gradient (DPG) algorithm was first introduced by \cite{silver2014deterministic} who propose an off-policy algorithm based on the actor-critic framework. We will go into detail about this approach in another chapter of the thesis. One year after the DPG paper got published, \cite{lillicrap2019continuous} combined the DPG method with the stability mechanisms presented in the DQN paper, to great success. As we will see in the next chapter that focuses on related work, nearly all efforts that involve  a control problem in the continuous action space utilize the improved version of DPG which is also referred to as DDPG \cite[p.~2]{lillicrap2019continuous}. The latest major accepted algorithm that is built on top of DDPG and tries to improve it, is the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm \cite[]{fujimoto2018addressing}. Besides, there exists an algorithm named Soft Actor-Critic (SAC) suggested by \cite{haarnoja2018soft} that combines the actor-critic framework of DDPG with a stochastic actor.
\par
As we noticed in the previous explanations and also in OpenAI's key paper list \cite[]{keypaperlist}, the release of milestone papers regarding the algorithms themselves stopped around 2018. The question arises as to what the main research topic trends were in the past three years because the overall hype around reinforcement learning certainly did not stop yet as the number of publications display - 1302 in 2018, 2140 in 2019 and 3256 in 2020 \cite[]{arxiv}. During our literature research, we notice an emerging trend towards offline reinforcement learning and the endeavor to make reinforcement learning methods more data efficient. \cite{levine2020offline} provide the research community with a great conceptual overview of the topic as a whole, including difficulties and open problems, while \cite{fu2021d4rl} introduce special data sets that are designed for benchmarking. \cite{fujimoto2019benchmarking} apply multiple offline reinforcement learning algorithms to the Atari game playing domain to compare the results to the original proposed DQN algorithm. All offline reinforcement learning algorithms are under-performing in comparison to the online DQN, with the strongest one being a discrete variant of Batch Constrained deep Q-Learning (BCQ) \cite[pp.~7-9]{fujimoto2019benchmarking}. Although offline reinforcement learning methods should learn an improved policy derived from the original behavior policy that sampled the data, BCQ is only able to match it and thus can be seen as robust imitation in this Atari setting \cite[pp.~7]{fujimoto2019benchmarking} which is not the desired outcome.
\par
However, imitation in general plays a major role in hard exploration problems, in which agents are unable to find even one successful decision sequence after billions of training steps. One example being the task of autonomous driving. \cite{le2022survey} describe in their related survey that applying imitation learning, a method of learning from expert (human) demonstrations, is more applicable to the driving task due to the complexity and safety critical nature (p.~1). The authors also list a wide range of related works, which in return utilize  constantly evolving methods of the realm of imitation learning (pp.~10-13). An overview of the advances of imitation learning can also be found in the paper proposed by \cite{zheng2021imitation}. Starting from one of the simplest approaches in the form of behavioral cloning which is also a core method used in this thesis, the authors also mention the latest developments of the three categories and their first presented algorithms, namely behavioral cloning \cite[]{alvinn}, direct policy learning (Generative Adversarial Imitation Learning (GAIL) \cite[]{ho2016generative}) and inverse reinforcement learning (Adversarial Inverse Reinforcement Learning (IRL) \cite[]{fu2017learning}). Some improved methods based on those three are e.g., CMILe \cite[]{tu2021closing}, DGAIL \cite[]{zuo2020deterministic}, and SQIL \cite[]{reddy2019sqil}. Besides, works such as the ones from \cite{paine2019making} and \cite{hester2017deep} combine the imitation aspect with the previously described DQN (learn to play Atari games based on pixel input) by suggesting methods called  Recurrent Replay Distributed DQN from Demonstrations (R2D3) and Deep Q-learning from Demonstrations (DQfD) respectively.
\par
A totally different approach is used in the recent work by \cite{chen2021decision} who abstract the reinforcement learning task as a sequence modeling problem and utilize the transformer architecture to eventually output the optimal actions. This shows the flexibility of the reinforcement learning framework and the underlying MDP because RL can easily be combined with all sorts of current state-of-the-art network architectures like transformers, GANs, CNNs, LSTMs etc. Especially the usage of transformers for the overall task of this thesis, the path prediction, might also be worth an investigation since works like the one from \cite{giuliari2021transformer} show their potential in forecasting (pedestrian) trajectories.
\par
Another focus of the latest research is the ability to extract features and learn directly from raw pixel input, dropping the need for manual feature extraction to shape the perfect state representation. \cite{srinivas2020curl} present a method called Contrastive Unsupervised Representations for Reinforcement Learning (CURL) that outperforms Rainbow (improved version of DQN) in the Atari domain by 30\% and almost matches the performance of SAC in the \textit{DM Control} environment which is not using raw pixel as input in contrast to CURL \cite[p.~7]{srinivas2020curl}. Displaying the fast pace in which research in the field of RL is done, it only took 20 days for \cite{kostrikov2021image} to relativize the achievements of CURL by showing that data augmentation is in fact the key to applying strong model-free algorithms like SAC to the task of learning from raw pixel input.
\par 
Last but not least there are aspirations to make reinforcement learning scale 
better on multi-machine-setups with one example being the framework IMPALA (Scalable Distributed Deep-RL with Importance Weighted
Actor-Learner Architectures) proposed by \cite{espeholt2018impala}.