Adding a mechanism to any reinforcement learning algorithm that endorses the exploration of the state and action space is crucial in order to try many decision paths and find a near optimal policy or \textit{the} optimal policy $\pi^*$. However, the policy that is constantly updated and improved during the learning process tries to fully exploit the environment as much as possible to collect the best returns that are currently achievable. This can lead to the policy getting stuck in a local minimum. This contrast is also known as the exploration–exploitation dilemma \cite[p.~3]{Sutton1998}.
\par
The simplest and widely used approach in order to address this problem is the usage of an $\epsilon$-greedy policy, meaning that with probability $\epsilon$ a non-greedy action is taken \cite[p.~100]{Sutton1998}. Nevertheless, this method is only suitable for discrete action where the policy selects from a set of actions. For the continuous case, some sort of noise has to be added to the suggested action value outputs of the actor. In the pseudocode, this is illustrated by the following line, where $\mathcal{N}$ is the component that generates the noise.
\par
\begin{equation*}
    a_t = \mu_\theta(s_t) + \mathcal{N}_t
\end{equation*}
\par 
The resulting value 
In the original DDPG Paper, \cite{lillicrap2019continuous} suggest the usage of a process that generates temporal correlated noise, specifically the \textit{Ornstein-Uhlenbeck process} (OU process) \cite[]{uhlenbeck1930theory}. The authors justify their decision with the argument that this process very efficiently explores physical control problems with inertia (pp.~4,11). An OU process mainly depends on three variables. The mean of the noise $\mu$, the scale of the noise $\sigma$ and the rate of mean reversion $\theta$. Fig. \ref{fig:ouProcess} illustrates the effect of different hyperparameter settings of $\sigma$ and $\theta$ while the noise oscillates around $\mu=0$. We can see that a high value for $\sigma$ like $0.6$ in conjunction with a moderate value for the mean reversion $\theta$, as displayed in \ref{fig:highSigmaNoise}, results in big swings of the generated noise. Bearing in mind that the last layer of the actor network outputs values between -1 and 1 due to the hyperbolic tangent activation function, we conclude that such a noise parameter setting is unsuitable as adding such a high magnitude of noise to a proposed action will lead to the action eventually getting clipped back to -1 to 1 in most cases, leaving the replay buffer with predominantly extreme action cases. Aiming for an oscillation with a moderate rate of mean reversion ($\theta = 0.15$) and a scale of noise depending on the exploration needed for the task ($\sigma$ between $0.05$ and $0.5$) is the better approach when using the OU as exploration tool for DDPG. In fact, the utilized library for reinforcement learning algorithms, \textit{Stable-Baselines3} (SB3) \cite[]{stable-baselines3}, also suggests a default value of $\theta = 0.15$. In their work, \cite{lillicrap2019continuous} make use of $\theta = 0.15$ and chose $\sigma$ to be $0.2$ (p.~11).
\begin{figure}[H]
    \begin{subfigure}{0.495\textwidth}
      \centering
      % include first image
      \includesvg[width=\textwidth]{images/ou_01_015.svg}  
      \caption{$\sigma = 0.1, \theta = 0.15$}
      \label{fig:lowSigmaNoise}
    \end{subfigure}
    \begin{subfigure}{0.495\textwidth}
      \centering
      % include first image
      \includesvg[width=\textwidth]{images/ou_06_015.svg}  
      \caption{$\sigma = 0.6, \theta = 0.15$}
      \label{fig:highSigmaNoise}
    \end{subfigure}
\medskip % create some *vertical* separation between the graphs
    \begin{subfigure}{0.495\textwidth}
      \centering
      % include first image
      \includesvg[width=\textwidth]{images/ou_03_001.svg}  
      \caption{$\sigma = 0.3, \theta = 0.001$}
    \end{subfigure}
    \begin{subfigure}{0.495\textwidth}
      \centering
      % include first image
      \includesvg[width=\textwidth]{images/ou_03_07.svg}  
      \caption{$\sigma = 0.3, \theta = 0.7$}
    \end{subfigure}
\caption{Influence of different values for $\mu$ and $\theta$ in the OU process.}
\label{fig:ouProcess}
\end{figure}

\par
In later publications that try to improve certain aspects of the DDPG algorithm, it is mentioned that the utilization of the—to some extent complex—OU process is not needed and is in turn replaced by fixed Gaussian Noise. An example of this notion are \cite{fujimoto2018addressing} who state in their work about TD3 (an improved version of DDPG which addresses potential function approximation errors) that they "found noise drawn from the Ornstein-Uhlenbeck process offered no performance benefits" (p.~7). \cite{barth2018distributed} come to the same conclusion who describe that they "experimented with correlated noise drawn from an Ornstein-Uhlenbeck" but they "found this was unnecessary and did
not add to performance" (p.~5).
\par
Although we do not utilize a ship simulation and therefore the environment cannot be directly interpreted as a physical problem, we still lay focus on the Ornstein-Uhlenbeck process as the main noise generator. The resulting ship trajectories from the AIS data still depend on the underlying ship dynamics, even though they are part of a more abstract layer.