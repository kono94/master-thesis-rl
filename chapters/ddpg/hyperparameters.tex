In almost every machine learning scenario, the choice and search of suitable hyper parameters play an important role. The idea of this sub chapter is to give an overview of all relevant parameters, their description and some sort of default values or ranges that are mentioned numerous times in publications and programming frameworks. Furthermore 
\par
\paragraph{Discount factor $\boldsymbol{\gamma}$.} The discount factor describes how far the agent should factor future decisions and their rewards into its current decision. The author's previous bachelor thesis dealt intensively with the effects of this factor on the learning process. In summary, it can be said that a discount factor tailored to the learning scenario leads to significantly faster convergence rates in contrast to choosing a value close to 1 which results in the highest amount of training steps needed to find the optimal policy. However, since the real-world problems to which RL is applied are usually very complex and require the agent's maximum foresight, reduced learning times are usually foregone and a discount factor very close to 1 is chosen. For that reason, $\gamma = 0.99$ is used throughout all experiments.

\paragraph{Network architecture} For both, the actor and critic, a suitable network architecture as in the amount of hidden layers and the number of neurons in each of them have to be defined. \cite{lillicrap2019continuous} make use of a structure for both components which is a hidden layer with 400 neurons and a second hidden layer with 300 neurons (p.11). The same structure can be found in many applied works on DDPG, which at the same time operate in very different use cases, see for example //TODO works with DDPG.
//TODO Edgardo first principle book about ML

\paragraph{Learning rates} The learning rates of the actor and critic networks, $\alpha_\mu$ and $\alpha_Q$, determine
how much the model parameters should adjust to the calculated gradients in order to minimize the network's loss function. \cite{lillicrap2019continuous} utilize step sizes of $\alpha_\mu = 10^{-4}$ and $\alpha_Q = 10^{-3}$ (p.~11) without explicitly explaining why the critic changes faster than the actor. Regarding this topic, \cite{degris2012off} make the assumption that the setting of a slower updating actor in comparison to the critic is "desirable because we  effectively want
a converged value function estimate for the current
policy weights" (p.~5). This assumption also gets referenced in the original DPG paper \cite[p.~6]{silver2014deterministic}.
\par 
The ratio of the learning rates is kept in all experiments while trying out different scales of magnitude.

\paragraph{Batch size} For each training step, the batch size determines the amount of transition tuples are taking out of the replay buffer and fed through the neuronal networks in order to perform one network update. \cite{lillicrap2019continuous} perform update with a batch size of 64 (p.~11), whereas the RL library used in this thesis, Stable Baselines 3 (SB3), suggests a default parameter of 100. Overall we do not experience major variances in the outcomes when using different values for the batch sizes that are in the range of 32 to 256. 

\paragraph{Soft update parameter $\tau$} Tau determines how much the weights of the target networks should adjust to the direction of the learned networks. \cite{lillicrap2019continuous} use a value of $\tau = 0.001$ (p.~11), whereas SB3 suggests a default value of 0.005. //TODO

\paragraph{Noise parameter} The mean of the noise is defined to be 0 because the activation function of the final output layer of the actor is the hyperbolic tangent (also referred to as "tanh") which clips the suggested actions to the range of -1 to 1. The parameters $\sigma$ for the scale of the noise and $\theta$ for the rate of mean reversion can vary a lot depending on the amount of exploration needed for a specific environment. In our work, we keep $\theta$ more or less constant at 0.15 while just experimenting with different $\sigma$ values between 0.1 and 0.5. 