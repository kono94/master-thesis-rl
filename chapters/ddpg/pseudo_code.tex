The pseudo code is cited from the DDPG paper proposed by \cite{lillicrap2019continuous} with the minor adjustment of adopting the definition of the timestep the next reward is given back to the agent to be $t+1$ as described in subchapter \ref{chap:rlframework} (p.~5):
\begin{algorithm}
    \caption{DDPG Algorithm}
    \begin{algorithmic}[1]
        \State Randomly initialize critic network $Q(s,a \mid \theta^Q)$ and actor $\mu(s \mid \theta^\mu)$ with weights $\theta^Q$ and $\theta^\mu$.
        \State Initialize target network $Q'$ and $\mu'$ with weights $\theta^{Q'} \leftarrow \theta^Q , \theta^{\mu'} \leftarrow \theta^\mu$ 
        \State Initialize replay buffer $R$
        \State \textbf{for} episode = 1, M \textbf{do}
        \Indent
            \State Initialize a random process $\mathcal{N}$ for action exploration
            \State Receive initial observation state $s_1$
            \State \textbf{for} t = 1, T \textbf{do}
            \Indent
                \State Select action $a_t = \mu(s_t \mid \theta^\mu) + \mathcal{N}_t$ according to the current policy and exploration noise
                \State Execute action $a_t$ and observe reward $r_{t+1}$ and observe new state $s_{t+1}$
                \State Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in $R$
                \State Sample a random minibatch of $N$ transitions $(s_i,a_i, r_{i+1}, s_{i+1})$ from $R$
                \State Set $y_i = r_{i+1} + \gamma Q' (s_{i+1}, \mu'(s_{i+1} \mid \theta^{\mu'})  \mid \theta^{Q'})$
                \State Update critic by minimizing the loss: 
                $L = \frac{1}{N} \sum_i{(y_i - Q(s_i, a_i \mid \theta^Q))^2}$
                \State Update the actor policy using the sampled policy gradient:
                \State \begin{equation*}
                    \nabla_{\theta^\mu} J \approx \frac{1}{N}
                    \sum_i{\nabla_a Q(s, a \mid \theta^Q) 
                    \mid_{s = s_i, a=\mu(s_i)} \nabla_{\theta^\mu} \mu(s \mid \theta^\mu) \mid_{s_i}}
                \end{equation*}
                \State Update the target networks:
                \State \begin{equation*}
                    \theta^{Q'} \leftarrow \tau \theta^Q
 + (1- \tau) \theta^{Q'}                \end{equation*}
 \begin{equation*}
                    \theta^{\mu'} \leftarrow \tau \theta^\mu
 + (1- \tau) \theta^{\mu'}                \end{equation*}
 
            \EndIndent
        \EndIndent
    \end{algorithmic}
\end{algorithm}