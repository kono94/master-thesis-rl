The replay buffer is a finite size cache $R$ which stores transition tuples $(s_t, a_t, r_{t+1},s_{t+1})$ \cite[p.~4]{mnih2013playing}. Speaking in terms of programming, the replay buffer can just be an array of fixed-size $N$. After each interaction between agent and environment another transition tuple is added to the array. If the array is filled up completely, the insert index gets reset to the first element, meaning that existing tuples are starting to get overwritten. It is worth noting that the replay buffer is not emptied throughout the learning process, so interactions over multiple episodes or even all past interactions can be present in the replay buffer at the same time. Each time the actor or critic is updated, a minibatch is randomly taken from the replay buffer to perform this update.
\par 
While conceptually trivial, the notion of a replay buffer in the context of reinforcement learning is first introduced by \cite{lin1992reinforcement} to confront multiple challenges arising from the usage of neuronal networks in this domain. \cite{mnih2013playing} list three reasons why a replay buffer is better or rather inevitable for learning in conjunction with neuronal networks in contrast to  the online variants.
\paragraph{Uncorrelated samples.} One of the most important  challenges emerges from the nature of the reinforcement learning setup and its sequential exploring.  Transition tuples get generated by the interaction between agent and environment step after step, resulting in highly correlated sequences of samples. Learning from those consecutive samples is inefficient because it would result in a high variance of updates \cite[p.~5]{mnih2013playing}. In this regard, \cite{lillicrap2019continuous} attest that "most of optimization algorithms assume that the samples are independently and identically distributed" (p.~4) and that this assumption no longer holds if "samples are generated from exploring sequentially in an environment" (p.~4).
\par
It may sound counterproductive at first to learn from transition tuples that get selected completely at random. In the beginning, the replay buffer is filled with "bad" decisions of the past. Even if the agent explores a better state related to an improved action selection or the addition of noise, due to the fact that the replay buffer can easily contain hundreds of thousands of transitions, it may take a huge amount of learning steps until those new and desired transitions are even part of one mini-batch that passes the critic network once. But as the previous paragraph explained, the usage of a replay buffer is mandatory to break correlations between samples. Additionally the concept of a pool to sample transitions from allows for the transformation of standard online Q-Learning to a more efficient variant because updates are based on batches instead of just the past interaction which inherently results in higher data efficiency.

\paragraph{Data efficiency.} A single experience from the agent interaction with the environment is present in the replay buffer as long as it gets overridden. If the buffer is large enough and memory is no issue, it is certainly possible that a transition tuple stays in the replay buffer for the whole learning process. Consequently, this single piece of experience is potentially used in multiple weight updates which results in higher data efficiency.

\paragraph{Avoiding feedback loops.} The third reason given by \cite{mnih2013playing} is an argument that endorses off-policy learning like Q-Learning. They argue that on-policy learning will produce unwanted feedback loops or that parameters could get stuck in poor a local minimum (p.~5). 
\par 
The term on-policy is used when only one policy and its current parameters determine the next data sample which is used to update exactly those parameters in return. On-policy methods do not make use of a separate behaviour policy. This ultimately results in a  binding between the maximizing action and the training distribution. A replay buffer on the other hand averages the training distribution because it includes many past states. By smoothing out learning,
oscillations or divergence in the parameters can be avoided \cite[p.~5]{mnih2013playing}.