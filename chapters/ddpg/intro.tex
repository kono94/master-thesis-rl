The goal of this chapter is to dive deep into an actor-critic algorithm to build up in-depth knowledge about every aspect of this reinforcement learning approach. As \cite{tim2018} points out, this is a necessary step towards actually using these methods to their full potential because in case of a failure the researcher has deep understanding and can bring up ideas on how to tweak certain aspects around the whole learning setup.
\par
To achieve this objective, we will take a very close look at the algorithm called \textit{Deep Deterministic Policy Gradient} (DDPG). Every element of this algorithm has its own subchapter where slices of the pseudocode from the original DDPG paper, published by \cite{lillicrap2019continuous}, will be included and discussed. The entire pseudocode with the minor adjustment of adopting the definition of the time step the next reward is given back to the agent to be $t+1$ as described in subchapter \ref{chap:rlframework}, is given by \cite[p.~5]{lillicrap2019continuous}:
\input{chapters/ddpg/pseudo_code}