After a profound literature research, we can divide the related work into three sub categories. Those include the topic of building a framework for anomaly detection in the maritime domain, the general application of machine learning in path planning or prediction and the latest achievements  in the field of deep reinforcement learning. 
%different approaches to reinforcement learning such as inverse- and offline reinforcement learning and the ongoing work done in the field of deep reinforcement learning.
\par 
\paragraph{Anomaly detection of vessel behaviour}
Building a maritime awareness system that operates in near real time by processing huge amounts of data from all kinds of different sources (AIS, radar systems, satellites, etc.) is not a trivial task. The work of \cite{tsogas2019geospatial} presents a system architecture named Geospatial Complex Event Processing Service (TRITON) that is capable of handling large volumes of incoming events using ActiveMQ as message broker and PostgreSQL as database which is optimized for storing and querying geospatial information. By using the Event Processing Language (EPL), the service is also able to detect abnormal vessel behaviour. Though the set of specific rules (vessel entering, exiting, crossing, moving away or approaching areas) have to be defined by domain experts \cite[p.~4]{tsogas2019geospatial} which is a critical limitation of the detector. The authors themselves are thus mentioning the usage of reinforcement and unsupervised learning in future work \cite[p.~9]{tsogas2019geospatial}.
\par 
The general approach of using geospatial data like AIS to learn motion patterns, that are eventually used to predict vessel paths and detect anomalies, is not a novelty. Instead of employing hand crafted rules, the highly cited work by \cite{ristic2008statistical} uses historical AIS data to extract motion patterns to train an anomaly detector by applying statistical methods such as adaptive kernel density estimation and particle filters. A motion pattern in this connection is defined by kinematic information including ship location and velocity but also by at least one mandatory attribute information - the ship's origin - to distinguish between overlapping motion patterns \cite[p.~2]{ristic2008statistical}. This could be an interesting factor when designing a suitable state representation to fulfill the Markov property.
\par 
In a recent summary of the related literature, \cite{zhang2020analysis} analyze the research trends of vessel abnormal behavior detection of the past ten years. They found that earlier work concentrated on the detection of just abnormal tracking positions mainly based on statistical methods by almost exclusively using movement data (position, speed, heading) of the perspective of a single ship (pp.~47-48). In contrast, more recent approaches try to detect anomalies in specific situations by including contextual information such as meteorological data, the interaction between ships, the perception of ship motion status (identification of fishing operations, ship towing push or ship gathering) and video data \cite[p.~50]{zhang2020analysis}. The usage of non-kinematic features should therefore be deflected to eventually construct a sophisticated anomaly detector that  truly has a sense of situational awareness. In their work, \cite{zhang2020analysis} conclude that detectors of abnormal vessel behaviour face major problems, such as poor comprehensiveness (cannot specify abnormality type and degree), difficulties with huge amounts of data due to the complexity of their algorithms and high false alarm rates because of subjectively chosen thresholds (p.~52).
\par 
As mentioned in chapter \ref{sec:problem}, the term \anf{anomaly} can mean many things even in the conjunction of AIS data. Detecting abnormal vessel behaviour solely based on the trajectory is the main focus of this thesis but there are other types as well. \cite{singh2020effectiveness} who are colleagues at the sister institute in Neustrelitz, near Berlin, focus on malicious and intentional AIS on-off switching (OOS) anomalies. In their paper they compare the performance of various AI techniques including support vector machines, k-nearest neighbours, decision trees, artificial neural networks and naive Bayesian towards detecting the AIS
OOS anomalies using real historical AIS data (p.~1). The results (99,9\% accuracy when using an artificial neural network \cite[p.~7]{singh2020effectiveness}) suggest that this specific task can be labeled as solved. 


\paragraph{Path prediction}
To predict the trajectories of vessels, \cite{liu2019vessel} use support vector regression (SVR) in conjunction with an algorithm called ACDE for  parameter optimization. ACDE stands for \anf{differential evolution based on adaptive control parameters} and is an improved version of the differential evolution algorithm which tackles stochastic parallel optimization and is based on evolutionary ideas in form of genetic algorithms \cite[pp.~1-3]{thangaraj2009simple}. In their work, \cite{liu2019vessel} also present algorithms on how to extract single trajectories from the AIS data using the unique maritime mobile service identification (MMSI) numbers and how to clear, de-noise and normalize the AIS data (pp.~9-11). As multivariable input or in the context of reinforcement learning this can also be called \textit{state}, the authors of this paper are using unix timestamps, longitude, latitude, course and speed over ground of the ship (p.~11). We can redefined their input as state definition for a better overview, where $CoG_t$ and $SoG_t$ are course and speed over ground respectively:
\begin{equation}
S_t = \{lon_t, lat_t, CoG_t, SoG_t, Unix_t\}
\end{equation}
Four of those previous states plus the timestamp of the next moment $T_{t+1}$ will be the true input for two separate SVR models, one predicts the longitude $lon_{t+1}$ and the other predicts the latitude $lat_{t+1}$ for just one moment in advance \cite[p.~11]{liu2019vessel}. Predicting just the next timestamp in advance, this method might be not suitable in terms of forecasting a complete vessel trajectory because of the major concern that this model will accumulate error if a prediction output is used as input for a larger forecasting window.
\par
In a recent published paper, \cite{venskus2021unsupervised} tackle the exact same topic of this thesis, that is the prediction of vessel trajectories based on historical AIS data in an unsupervised manner. They use an LSTM autoencoder that learns to reconstruct vessel paths for the next $X$ timestamps (p.~724). Furthermore the authors utilize a method proposed by \cite{cruz2019} to eventually generate a prediction region that is learned by two supplementary LSTM autoencoders in addition to the  most like-hood forecast of the original autoencoder that learns single trajectories (p.~725). Predicting a potential region that vessels under normal behavior would navigate in, makes anomaly detection trivial by just checking if the prognosticated path is inside the region that is considered \anf{normal}. Nevertheless a major constraint of this method is the needed input sequence of past $X$ timestamps to start forecasting the next $X$ timestamps.
\par 
\cite{perera2012maritime} introduce a framework to monitor maritime traffic by constructing three modules. The first module consists of an artificial neural network that detects and tracks multiple vessels while the second and third modules are used to estimate the vessel states and forecast the navigational trajectory by using an extended Kalman filter \cite[p.~1]{perera2012maritime}. 
\par 
A complete different approach is presented by \cite{6198334} who take advantage of the genetic algorithm in conjunction with randomly generated Bezier curves to solve the path planning of  autonomous unmanned aerial vehicles (UAVs) in previously defined environments~(p.~1). Besides they achieve quasi-linear speedup in relation to the number of CPU cores by using a parallel programming paradigm called \anf{single-program,
multiple-data} (pp.~7-8). Although the authors themselves state that the planning takes 10 seconds while running on eight cores in parallel, they still make the conclusion that \anf{real-time path planning for UAVs is possible} \cite[p.~9]{6198334}. However, we see this computational cost as critical limitation of classic genetic algorithms in general. Even though vessel planning cuts one dimension as the prediction of UAVs paths takes place in a 3D domain, we still make the assumption that this approach is not suitable for future visions of calculating vessel paths of hundreds of ships in near real-time. 
\par 
Entering the field of deep reinforcement learning, we can notice that most works are related to the control or path planning of unmanned vehicles (land, water or air) without focusing specifically on anomaly detection but rather control. \cite{etemad2020using} and \cite{zare2021continuous} for example utilize the classic Floydâ€“Warshall algorithm as path-planner and apply deep Q-learning or Deep Deterministic Policy Gradient (DDPG) respectively to tackle obstacle avoidance for unmanned surface vehicle (USV). In this case, deep reinforcement learning is used to evaluate the current situation (local view) on the USV's path and makes short-term decisions to avoid collisions (p.~8).

\par
The usage of Deep RL not only as supportive mechanism for short-term decision making but rather as full control algorithm for intelligent vehicles is the main field of application for reinforcement in general and, 
as a consequence, is strongly present in the literature. \cite{wang2018reinforcement} for instance take advantage of DDPG to control an autonomous underwater vehicle (AUV) in an under-ice environment that eventually learns the long-term reward of reducing the field uncertainty and the AUV mobility cost (p.~17). Another example is the work of \cite{s18092905} whose agent represents a land vehicle. They extract an abstract model of the real environment to then teach the agent driving maneuver like overtaking, following a curve, ramp driving as well as staying in lane (pp.~1,4). As the agent is trained by the DDPG algorithm, he implicitly learns to drive on or near the desired path. This is mainly accomplished by defining the desired path and using the difference between the current posture and the desired one as part of the reward signal \cite[p.~4]{s18092905}. After training the agent in the virtual environment, the authors transfer their model and their \anf{end-to-end trajectory planning} (p.~17) to a real world scenario by letting the agent control a self driving bus (pp.~18-19).
\par
Staying in the maritime domain, we can identify the inventiveness of crewless autonomous ship systems as the closest research topic to the one of this thesis. In this field, \cite{s20020426} present a paper that uses the actor-critic algorithm DDPG to select actions which navigate a vessel on the fastest way to the target destination while also taking into account ship encounters (pp.~13-14). The important parts of their path planning module are the environment and the reward function. For the environment they choose a combination of two components. The first one is the \anf{Ship Action Controller} which converts the model output to an actual action that the unmanned ship should take (p.~15). The second one is the so called \anf{Ship navigation information fusion module} which is able to receive data from GPS, AIS, depth sounder and anemometer to construct a state that holds information about the ship itself, obstacles and the target point (p.~10). The reward signals is built by transforming the crew's experience and the navigation rules defined by COLREGS \cite[]{COLREG} into navigation restriction area \cite[p.~14]{s20020426}. Crossing a restricted area or colliding with an obstacles results in a negative reward. Additionally the inverse of the distance between the agent and the target point is also granted as reward signal \cite[p.~14]{s20020426}. The results displayed by the authors are promising as the agent not only learns to get the destination on the shortest path while avoiding obstacles but also reacts and adopts to multiple ships on its path
\cite[pp.~21-26]{s20020426}.
\par 
Instead of learning online by interacting with an environment it is also possible to learn offline by using historical AIS data. This approach is followed by \cite{westerlund2021learning} who uses offline reinforcement learning. The goal of his master thesis is to make an agent (the vessel) learn to get to a destination based on previously sampled data from a simulator. The simulator outputs a series of AIS data points representing the taken vessel trajectories which is used as data set. Here, the state is defined by latitude, longitude, Speed over Ground, Course over Ground, and most importantly the rudder angle while the action space only consists of the heading the ship should have at the next timestamp \cite[pp.~30-33]{westerlund2021learning}. To fully build the necessary tuples that get fed into the replay buffer $(s_t, a_t, r_t, s_{t+1})$, the author calculates the reward based on the difference between the current position and the targeted one (p.~27). Although the presented results show that it is generally possible to apply offline reinforcement learning to the path planning problem of vessels, the author himself states that performance is \anf{limited by the size and diversity of the dataset}(p.~40) as the generalization outside the trained area is deficient.
\par
The previously cited works are better categorized as \anf{path finders} which means that their reward function is built around finding the best or potential new trajectory to a destination. This lies in the nature of reinforcement learning being mainly used in control problems that try to come up with a policy that has learned the underlying dynamics of an environment. Even if the training data is not sampled online but generated in the past, like in the work from \cite{westerlund2021learning}, the reward function is still defining the semantic of the goal which is very different from the goal of learning representative trajectories for future anomaly detection. For this specific setting the term \anf{path follower} might be more fitting. We found a paper from \cite{martinsen2018curved} which focuses exactly on this topic that is the following of a predefined curved path by a vessel. In their work they use DDPG and a Gaussian reward function that is mainly built from a cross-track error which tells how far away the vessel is from the predefined path \cite[p.~3]{martinsen2018curved}. 


\paragraph{Deep Reinforcement Learning}
This sub chapter gives an overview of the latest developments and achievements in the research field of deep reinforcement learning, starting with the key paper that is responsible for the re-ignition and the explosion of interest in reinforcement learning, the work from \cite{mnih2013playing, mnih2015human}.
\par 
\cite{mnih2013playing} propose an algorithm called Deep Q-Network
(DQN) that combines Q-Learning with a neural network which is used to estimate the Q-values instead of using a look-up table. One of the reasons the paper got so much attention is due to the fact that the algorithm learned to play seven Atari games on human like level just based on the pure pixel input without changing the architecture (p.~7-8). The main difference between the presented method by \cite{mnih2013playing} in contrast to past approaches of using neural networks in conjunction with reinforcement learning, is the introduction of an experience replay and a target network to ensure stability during the training process. We go into detail of those components later on in this thesis.
\par
Four years after the initial publication of DQN, \cite{hessel2018rainbow} combine the suggested improvements to DQN by the research community to propose an algorithm called \anf{Rainbow}. Although DQN and Rainbow can be seen as milestone methods, they are still only capable of handling discrete action spaces. Instead of estimating Q-values, policy gradient methods represent the policy by a parametric probability distribution that either stochastically or deterministically selects an action. Those methods are used to be able to operate in the continuous action space as this often a mandatory requirement for example in robotic control. Examples for algorithms that are labeled as the stochastic case are Proximal Policy Optimization (PPO) \cite[]{schulman2017proximal} and Trust Region Policy Optimization (TRPO) \cite[]{schulman2017trust}.
\par 
A deterministic policy gradient (DPG) algorithm was first introduced by \cite{silver2014deterministic} who propose an off-policy algorithm based on the actor-critic framework. We will go into detail of this approach in a chapter of the thesis. One year later after the DPG paper got published, \cite{lillicrap2019continuous} combined the DPG method with the stability mechanisms presented in the DQN paper, with great success. As we have already seen in the previous sub-chapter, nearly all works that involve  a control problem in the continuous action space utilize the improved version of DPG which is also referred as DDPG \cite[p.~2]{lillicrap2019continuous}. The latest major accepted algorithm that is built on top of DDPG and tries to improve is, is the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm \cite[]{fujimoto2018addressing}. Besides, there exists an algorithm named soft actor-critic (SAC) suggested by \cite{haarnoja2018soft} that combines the actor-critic framework of DDPG with a stochastic actor.
\par
As we noticed in the previous explanations and also in OpenAI's key paper list \cite[]{keypaperlist}, the release of milestone papers regarding the algorithms themselves stops around 2018. The question arise as to what the main research topic trends are in the past three years because the overall hype around reinforcement learning certainly did not stop yet as the number of publications display - 1302 in 2018, 2140 in 2019 and 3256 in 2020 \cite[]{arxiv}. During our literature research, we notice an emerging trend towards offline reinforcement learning and the strive to make reinforcement learning methods more data efficient. \cite{levine2020offline} provide the research community with a great conceptual overview of the topic as a whole, difficulties and open problems while \cite{fu2021d4rl} introduce special data sets that are designed for benchmarking. \cite{fujimoto2019benchmarking} apply multiple offline reinforcement learning algorithms to the Atari game playing domain to compare the results to the original proposed DQN algorithm. All offline reinforcement learning algorithms are under-performing in comparison to the online DQN with the strongest one being a discrete variant of Batch Constrained deep Q-Learning (BCQ) \cite[pp.~7-9]{fujimoto2019benchmarking}. Although offline RL methods should learn an improved policy derived from the original behaviour policy that sampled the data, BCQ is only able to match it and thus can be seen as robust imitation in this Atari setting \cite[pp.~7]{fujimoto2019benchmarking} which is not the desired outcome.
However, imitation in general plays a major role in hard exploration problems in which agents are unable to find even one successful decision sequence after billions of training steps. The works from \cite{paine2019making} and \cite{hester2017deep} address this problem by suggesting methods called  Recurrent Replay Distributed DQN from Demonstrations (R2D3) and Deep Q-learning from Demonstrations (DQfD) respectively.
\par
A different approach is used in the recent work by \cite{chen2021decision} who abstract the reinforcement learning task as a sequence modeling problem and utilize the transformer architecture to eventually output the optimal actions.
\par
Another focus of latest research is the ability to extract features and learn directly from raw pixel input dropping the need for manual feature extraction to shape the perfect state representation. \cite{srinivas2020curl} present a method called Contrastive Unsupervised Representations for Reinforcement Learning (CURL) that outperforms Rainbow (improved version of DQN) in the Atari domain by 30\% and almost matches the performance of SAC in the DMControl environment which is not using raw pixel as input in contrast to CURL \cite[p.~7]{srinivas2020curl}. Displaying the fast pace in which research in the field of RL is done, it only took 20 days for \cite{kostrikov2021image} to relativize the achievements of CURL by showing that data augmentation is in fact the key to apply strong model-free algorithms like SAC to the task of learning from raw pixel input.
\par 
Last but not least there are aspirations to make reinforcement learning scale 
better to multi-machine-setups with one example being the framework IMPALA (Scalable Distributed Deep-RL with Importance Weighted
Actor-Learner Architectures) proposed by \cite{espeholt2018impala}.