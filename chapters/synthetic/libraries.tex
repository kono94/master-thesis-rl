Although we implemented the DDPG algorithm from scratch and tested its correctness by applying it to the classic control problem \textit{MountainCarContinuous-v0} \cite[]{moore1990efficient}, we will still make use of sophisticated external libraries for deep-, reinforcement- and imitation learning. Those libraries are exclusively open-source, actively maintained by thousands of contributors, and well known in the research community (based on respective numbers from GitHub in contributors and stars). To give an overview of the most important libraries used in this thesis, we list and describe their functionality in the upcoming subchapters.

\subsubsection{PyTorch}
\textit{PyTorch} \cite[]{NEURIPS2019_9015} offers the foundation of handling tensor computation and the building of deep neuronal networks. Being implemented in C++ with strong GPU acceleration, its Python API is not as high-level as, e.g., the one of \textit{Keras}. On the one hand, this might discourage certain end users because more boilerplate code has to be written. On the other hand, its  “everything is a just a program” philosophy \cite[p.~4]{NEURIPS2019_9015} of treating layers, optimizers, activations, or data loaders as functions which can be composed in any arbitrary way, allows for great flexibility when it comes to assemble complex neuronal network architectures (e.g., loops or recursive functions). In order to compute the gradients of arbitrary models and user defined functions, PyTorch uses a method called \textit{automatic differentiation} \cite[p.~5]{NEURIPS2019_9015}. In short, this method saves all operations and interim tensors of the forward pass in a \textit{direct acyclic graph} (DAG) to calculate the respective gradients and apply the chain rule during backprogragation. PyTorch records the series of user functions and assembles the DAG at runtime by operation overloading.

\subsubsection{Stable-Baselines3}
\textit{Stable-Baselines3} \cite[]{stable-baselines3} provides reliable implementations of popular reinforcement learning algorithms. It is built on top of PyTorch, which was a decision made by the authors supported by a twitter survey of its users \cite[]{pytorch-survey}. Moreover, the DLR-MI sister institute \textit{Institute of Robotics and Mechatronics} (DLR-RM) is actively involved in maintaining this library and actually hosts the project on their official GitHub page.
\par
We mainly use Stable-Baselines3 for its implementation of DDPG. It also allows us to easily swap out and test more recent algorithms for continuous action spaces like \textit{Twin Delayed DDPG} (TD3) and \textit{Soft Actor-Critic} (SAC). 

\subsubsection{imitation}
The \textit{imitation} library \cite[]{wang2020imitation} implements popular imitation learning algorithms on top of Stable-Baselines3 and even gets referenced by the official SB3 documentary, as well as in the Stable-Baselines3 publication \cite[p.~2]{stable-baselines3}. In its current state, this library includes \textit{Behavioral Cloning}, \textit{DAgger}, \textit{Maximum Causal Entropy Inverse Reinforcement Learning}, \textit{Adversarial Inverse Reinforcement Learning}, \textit{Generative Adversarial Imitation Learning}, and \textit{Deep RL from Human Preferences}.

\subsubsection{MovingPandas}\label{subchap:movingPandas}
\textit{MovingPandas} \cite[]{graser2019movingpandas} is built on top of \textit{GeoPandas} \cite[]{kelsey_jordahl_2020_3946761} and offers classes and functions for movement data analysis. To be more precise, we use MovingPandas to transform raw AIS records into trajectories and make use of its powerful high-level functionalities such as \textit{Splitters} and \textit{Cleaners} to preprocess and generate our datasets of real-world vessel paths.