Although we implemented the DDPG algorithm from scratch and tested its correctness by applying it to the classic control problem "MountainCarContinuous-v0" \cite[]{moore1990efficient}, we will still make use of sophisticated external libraries for deep-, reinforcement- and imitation learning. Those libraries are exclusively open-source, actively maintained by thousands of contributors and well known in the research community (based on respective numbers from Github in contributors and stars). To give an overview of the most important libraries used in this thesis, we list and describe their functionality in the upcoming subchapters.

\subsubsection{PyTorch}
PyTorch \cite[]{NEURIPS2019_9015} offers the foundation of handling tensor computation and the building of deep neuronal networks. Being implemented in C++ with strong GPU acceleration, its Python API is not as high-level as e.g. the one of \textit{Keras}. On the one hand this might discourage certain end user because more boilerplate code has to be written. However, on the other hand its  “everything is a just a program” philosophy \cite[p.~4]{NEURIPS2019_9015} of treating layers, optimizers, activations or data loaders as functions which can be composed in any arbitrary way, allows for great flexibility when it comes to assemble complex neuronal network architectures (e.g. loops or recursive functions). In order to compute the gradients of arbitrary models and user defined functions, PyTorch uses a method called "automatic differentiation" \cite[p.~5]{NEURIPS2019_9015}. In short, this method saves all operations and interim tensors of the forward pass in a direct directed acyclic graph (DAG) to calculate the respective gradients and apply the chain rule during backprogragation. PyTorch records the series of user functions and assembles the DAG at runtime by operation overloading.

\subsubsection{Stable-Baselines3}


\subsubsection{imitation}
The \textit{imitation} library \cite[]{wang2020imitation} implements popular Imitation Learning algorithms on top of Stable-Baselines3 and even gets referenced by the official SB3 documentary. In its current state, this library includes Behavioral Cloning, DAgger, Maximum Causal Entropy Inverse Reinforcement Learning, Adversarial Inverse Reinforcement Learning, Generative Adversarial Imitation Learning and Deep RL from Human Preferences.

\subsubsection{MovingPandas}\label{subchap:movingPandas}
\textit{MovingPandas} \cite[]{graser2019movingpandas} is built on top of \textit{GeoPandas} \cite[]{kelsey_jordahl_2020_3946761} and offers classes and functions for movement data analysis. To be more precise, we use MovingPandas to transform raw AIS records into trajectories and make use of its powerful high-level functionalities such as "Splitters" and "Cleaners" to preprocess and generate our final dataset of real-world vessel paths.