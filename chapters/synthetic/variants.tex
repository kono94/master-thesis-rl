Our custom environment implements the popular \textit{gym} interface \cite[]{gym}. This interface is the de facto standard when it comes to creating RL environments because all major RL and IL libraries (\textit{Stable-Baselines} \cite[]{stable-baselines3}, \textit{KerasRL} \cite[]{plappert2016kerasrl}, \textit{Acme} \cite[]{hoffman2020acme}, \textit{Spinning Up} \cite[]{SpinningUp2018}, etc.) implement their algorithms towards this interface.
\par
The interface itself requires the programmer to define an action- and observation space as attributes. In order to test which features have to be present in the observation space and how the agent reacts to a one- or two-dimensional action space, we define multiple variations of the environment. Besides the characterization of the action- and observation space, the interface includes a self explaining \textit{reset}-function and a \textit{step}-function which takes in an action as parameter and returns the next state, the reward, a boolean flag whether the terminal state is reached and optional information. Therefore, the \textit{step}-function represents the transition dynamics of the environment. We will try different approaches to the task of actually being able to predict future states of trajectories which in return is manifested in deviating transition dynamics and \textit{step}-function thus resulting in another set of variants.

\paragraph{States and actions.} Choosing the right state representation is mandatory for the agent to take the correct actions. In this case, the MDP framework assumes that the state representation fulfills the \textit{Markov} property, which implies that a single state in the present (no history involved) retains all relevant information for the agent to distinguish between different environment snapshots, to be able to make suitable decisions. We will intentionally break this assumption by constructing one variant that has a state representation of just $\{x,y\}$ to demonstrate the importance of the \textit{Markov} property.
\par
To make different trajectories distinguishable by the state representation, the direction has to be added as well. For the simple case, we want the agent to solely select its direction at each time step and let the environment specify the perfect speed value. A more demanding variant adds the speed component to the state representation and the action space, making them four- respectively two-dimensional.
\par
The analysis and discussion about the results will show that even a state representation of position, speed, and direction might not fulfill the assumption of a Markovic state. Hence, we will also try state representations that include the current distance to the ground-truth (GT), as well as a representation that contains the current time step. Incorporating a time component into the state model was already mentioned in the related work section of this thesis (see chapter \ref{chap:relatedWork}) in context of the work proposed by \cite{liu2019vessel} who include a Unix timestamp to their state representation.

\begin{equation}
\begin{aligned}
    S1 := S_t &= \{x_t,y_t\}
\\
    S2 := S_t &= \{x_t,y_t, direction_t\}
\\
    S3 := S_t &= \{x_t,y_t, direction_t, speed_t\}
\\
    S4 := S_t &= \{x_t,y_t, direction_t, speed_t, distance_t\}
\\
    S5 := S_t &= \{x_t,y_t, direction_t, speed_t, t\}
    \end{aligned}
\end{equation}\label{stateRepresentation}

For clarity, we defined the labels for the action features as "course" and "tempo" to better indicate that they are independent of the "direction" and "speed" features of the state representation.
\begin{equation}
\begin{aligned}
    A1 := A_t &= \{course_t\}
\\
    A2 := A_t &= \{course_t, tempo_t\}
    \end{aligned}
\end{equation}

\paragraph{Transition dynamics.} The transition dynamics are the  core of every environment. Normally, the respective \textit{step}-function is well-defined and meant to be immutable. Meaning, that the environment dynamics are set before the application of any RL algorithms and those algorithms actually try to solve the sequential decision problem that arises by the interaction with the environment. In our case, we try to squeeze the task of predicting future vessel trajectories into the RL framework, trying to approach the task differently by making an agent interact and select at every time step instead of, e.g., looking at the problem as a time series prediction exercise. Hence, we are in charge of constructing artificial transition dynamics that best fit to our problem. It is even imaginable and most likely unavoidable that an agent trains in one environment but uses its learned policy in another environment with different transition dynamics that depict the real-world use case. This is due to the fact that training has to include some sort of guiding historical ground-truth trajectories, which are not present in the actual application of predicting paths.
\par
While looking at the overall task again, we construct a set of different transition dynamics. The ground-truth data consists of starting points and functions that generate direction values at every time step. We call this instance GT for ground-truth, which shares the same concept as the agent. The GT and the agent both have a current position, which in conjunction with the proposed direction is used to calculate the next position. The histories of positions which are unknown to the agent are the ground-truth trajectory and the generated trajectory by the agent. If we ignore the speed component for this explanation, the agent receives an initial state $\{x_0,y_0,heading_0\}$ which is identical to the GT. Next, the agent selects a value for the direction it should move ($a_0=\{direction_0\}$). Internally, the next GT position is calculated by using its current x, y coordinates at $t=0$ and the output of the curve-function for $t=1$. The next agent position is derived from ($x_0, y_0$) and the proposed direction from the action. For both calculations, an internal value for the current speed is utilized. The next state consists of the agent's current x and y values and the current heading, which in fact is the same as the previously proposed direction. Consequently, the agent tries to predict the next output value of the underlying curve function at $t+1$ to move in the same direction as the GT in order to stay as close as possible to the current GT position.
\par
Our assumption is that, after the training phase, the agent successfully learned to map $\{x_0,y_0,heading_0\}$ states to the corresponding GT direction values of the future next state. This policy is then used in a customized version of the environment which does not include GT data but uses the same routine to calculate next points, given the current agent position and the suggested direction. In other words, the agent's policy is trained by learning the same underlying policy of the GT in order to generate representative trajectories.
\par
Passing through the journey of creating multiple experiments and different approaches, we also take a deviating route to the one mentioned in the previous paragraph. Instead of returning the agent's next position and heading, the \textit{step}-method will return the next state of the GT. At first this may sound contradictory to what we discussed regarding the RL framework because the agent does not fully experience the potential drawbacks of past decisions. Even if the agent selects an action which leads to the agent drifting heavily apart from the GT, the next state will still be an optimal state of the GT. The reason behind this approach is the idea, that the agent learns the desired mapping more efficiently since suboptimal decisions do not accumulate error and provoke bad states which the agent is unable to recover from. As a matter of fact, the mechanism of recovery, meaning that the state space is expanded by, for example, a feature that tells the agent how far away it is from the GT and in what direction, is impossible as a consequence of the missing GT in the real-world scenario. The major speculation that is up for testing is the possible transfer of the trained policy which—in the training phase—receives next states from the GT, to a production environment without GT and adjusted transition dynamics that return states purely based on the agent's kinematics (except for the initial state).