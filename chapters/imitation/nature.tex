When we do write "learn like a human" as in the preceded subchapter, we refer to \textit{the} underlying motivation of reinforcement learning and the fact that it draws inspiration from psychological learning theories \cite[p.~341]{Sutton1998} and neuroscience \cite[p.~377]{Sutton1998}. Reinforcement learning is heavily influenced by the way humans and animals learn and interact with their real-world environment, based on for example chemical reward signals such as dopamine \cite[p.~383]{Sutton1998}. Taking nature or biology as blueprint to build artificial intelligence is not exclusive to the rl-framework. Neuronal networks for instance try to essentially copy the way our human brain works, with components even named "neurons" which are connected to each other (like synapses do in our brain) that reach different levels of activity (activation function).
\par
In our opinion it is a great advantage of reinforcement learning in general that it is so closely related to psychology and nature as a whole. It allows the researcher to take a step back from the formalism, algorithms and programming part in an effort to look at the problem from a different perspective. When we do take the example of playing Dota 2 from chapter \ref{subsec:weak}, there is a significant difference as to how we as human beings would approach such a task. One major advantage is the accumulated knowledge that we experienced in our prior lifespans. We do not start from zero, concepts like "enemies", "abilities" or "movement prediction" are already present that can be utilized and adopted in upcoming learning tasks. A whole research area focuses exactly on this ability, to learn from one task and transfer this knowledge to a different task - \textit{Transfer Learning} - which is out of scope for this thesis.
\par
Another great tool at our disposal is our talent to learn from others. In the case of Dota 2, these would be more experienced and better players that we face during our time playing the game. Our exceptional ability to mimic the behaviour of others (body movement, strategy, tool usage, etc.) might be inconspicuous to ourselves but is one key factor that we are able to solve new problems in relative short periods of time. It is also essential for the development of children as they usually start imitating their parents at an age of one to two \cite[p.347]{wood2013whom}. When trying to transfer this phenomenon from nature into the realm of machine learning, we find ourselves in an area summarized under the term \textit{Imitation Learning} (IL). It is the general approach of learning from an expert demonstrator in a supervised manner and sort of mimicking and generalizing its behaviour. Here, the demonstrator can either be a human expert that is familiar with the domain/task or an already trained artificial agent with a near optimal policy.
\par
RL and IL are very closely related, because they both try to solve problems that can be formulated as Markov Decision Processes and act within the same framework (see chapter \ref{chap:rlframework}). Additionally there even exist IL methods that actually utilize classic RL algorithms like DDPG after inversely reconstructing an underlying reward function (such a method is presented in chapter \ref{subchap:inverse}). What greatly separates IL from interactive RL methods is the fact that the reward function $r(s_t, a_t)$ does not have to be provided in the case of IL. As discussed in chapter $\ref{subchap:weak}$, the reward function is the most crucial component in order to tell the agent what desired goal it should reach. Nonetheless, this reward signal function is hand-crafted and can be hard to design especially if the researcher wants to "guide" the agent by utilizing non-sparse rewards that by some means defined sub-goals. (//TODO non-sparse rewards) IL in its simplest form drops the concept of reward functions entirely and makes the assumption that it is easier to demonstrate the desired behaviour than it is to specify the goal-oriented behaviour by a separate reward function as in RL. Therefore IL is also known as \textit{learning from demonstrations}. These demonstrations are recorded interactions from an expert with the environment $\tau = \{(s_t, a_t), (s_{t+1}, a_{t+1}), (s_{t+2}, a_{t+2}), \dots\}$ and are used to train a policy in a supervised manner that is way more efficient than classic interactive RL because trajectories of optimal behaviour are already given and do not have to be explored first.
\par
Although the upsides of IL (no hand-crafted reward function and very efficient policy learning) sound promising, it also has a major weakness in that in the naive approach violates the common "independent and identically distributed random variables" (i.i.d) assumptions made in statistical learning \cite{ross2011reduction} because the distributions of states encountered are different for the expert and trained agent. Intuitively this is because the train set overwhelmingly consists of "perfect" expert trajectories and if the agent makes just one mistake in a real rollout it will end up in a state that is totally unknown. The agent is consequently unable to recover from such a "bad" state.


\subsection{Reinterpreting the task}
The overall objective of this thesis, namely the vessel path prediction, can be interpreted in a different way when having imitation learning in mind. Being influenced by related work in the field of deep reinforcement learning and its applications in the maritime domain (autonomous vessels or RUVs, trajectory predicting with DDPG), we were first heavily focused on RL methods such as DDPG that directly try to solve control problems by maximizing the discounted sum of rewards. The basic idea is to define a reward function such as one that assigns positive rewards in case the agent stays near the true ship position. This task however seems more like a path following problem where an agent actually tries to mimic the behaviour of a captain in order to generate similar or almost identical trajectories. One major assumption is that if the agent actually learns the underlying behavior policy of the experts to imitate all the different trajectories of the training set, it successful learns to generalize and is also able to generate vessel paths based on slightly different states that it did not encounter during training.
//TODO generalize advantage of overall task due to broad range of historical vessel path and set sea routes?!