The first occurance of a technique that can be labeled as Behavioral Cloning is documented in the popular work of \cite{alvinn} and his proposed system called ALVINN (Autonomous Land
Vehicle In A Neural Network). This system first introduces neuronal networks to the domain of autonomous road followers. Recording steering maneuvers from a human actually driving a vehicle, it tries to mimic the human behaviour of controlling a vehicle based on the input of a laser range finder and images taking by a camera \cite[p.~2]{alvinn}. Training of the underlying neuronal network is done on-the-fly after an additional step of generating augmented images that correspond to the road curving to the left or to the right. Those synthetic images are created with the aim of dealing with the weaknesses when solely expert transitions are used as discussed in subchapter \ref{subchap:weak}. A human driver will more or less stay in the middle of the road without entering bad states of almost drifting away or hitting the side walk, hence the need for distorted images to eventually be able to generalize well enough and recover from a broad range of states. In the end, the vehicle was capable of autonomously following the road with a speed of 1 meter per second \cite[p.~7]{alvinn}. The system was limited by the computational power at that time but still showed the practicability of Behavioral Cloning and Imitation Learning in general.
\par
The results of the synthetic experiments in chapter \ref{chap:synthetic} as well as the results of the real-world path prediction in chapter \ref{chap:realworld} will show that Behavioral Cloning (BC) is far more suitable for the overall task than DDPG. Besides its superior performance, BC is also more explainable and the outcomes when exposing it to different state representations are close to be forecastable. 
\par
The main reason for the explainability originates from the its simplicity and its approach to reduce the task of imitating the behaviour of an expert to a supervised learning problem. In this connection, a neuronal network parameterized by $\theta$ learns to map states to actions as closely as possible to the expert policy $\pi^*$. That is, it aims to minimize expected distance $L$ between actions suggested by the trained policy and the expert actions for all states encountered in the train set $(s, a^*) \sim P^*$, where $P^*(s|\pi^*)$ is the state distribution of the expert policy \cite[]{le2022survey}. The optimal policy is then found as:

\begin{equation}
\argmin_\theta \EX_{(s,a^*) \sim P*} L(a^*, \pi_\theta(s)).
\end{equation}

A reward function is not required which takes away the uncertainty of modelling a applicable and task specific feedback signal. The state distribution $P^*(s|\pi^*)$ is given by the sampled demonstrations. Consisting of state-action pairs, those expert trajectories are generated either by recording a human who interacts with the environment, an already trained and near optimal policy or - as used in this thesis - a customized environment that ignores actor inputs and constructs perfect state-action pairs based on given ground truth data that includes the optimal next actions.