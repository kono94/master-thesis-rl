As already mentioned in subchapter \ref{subchap:nature}, there is another branch of methods in the field imitation learning that is summarized under the term \textit{Inverse Reinforcement Learning}. Generally, this approach consists of two steps. First, a reward function is reconstructed from the expert demonstrations alone which is then used in the next step to apply a standard reinforcement learning algorithm of choice (like DDPG) to the problem.
\par
The computational task of inversely extracting a reward function from set of transition tuples is originally proposed and defined by \cite{russell1998learning}. Although inverse reinforcement learning is a promising research field that drops the need of reward engineering and is much more sophisticated than behavioral cloning, we will refrain from diving deep into the theories behind it. The reasons being that it would overextend the scope of this thesis and the fact that behavioral cloning shows good performances in the conducted experiments. Nethertheless, we briefly introduce the reader to two popular algorithms that are also part of the synthetic experiment setups while refering to excellent overview of the evolution and latest algorithms in the field of imitation learning by \cite{zheng2021imitation}.
\par
The two algorithms we like to mention are  \textit{Adversarial Inverse Reinforcement Learning} (AIRL) \cite[]{fu2017learning} and \textit{Generative Adversarial Imitation Learning} (GAIL) \cite[]{ho2016generative}. Even though they both learn adversarial, meaning that they utilize a generator- and a discriminator network as proposed by \cite{goodfellow2014generative}, they differ from the overall strategy to solve the task of learning a near optimal policy based on expert demonstrations. AIRL adversarially learns the 
reward function from the underlying transition dynamics of the expert tuples to and then solve the reward ambiguity problem \cite[pp.~4-5]{fu2017learning}. A big advantage of this method is that the extracted reward function is generalizable and portable \cite[p.~1735]{wang2020deep}.
\par
GAIL on the other does not explicitly learn a reward function. The idea is to adopt the principle of a \textit{Generative Adversarial Network} (GAN) to the domain of imitation learning in order to directly extract the expert policy. In short, a GAN consists of a generator network $G$ that tries to confuse the discriminator network $D$ which in return should predict whether a sample is part of the training data or a fake generated by $G$ \cite[p.~1]{goodfellow2014generative}. When $D$ is unable to distinguish between data generated by $G$ from the true data, then $G$ successfully learned the true data distribution. In the context of imitation learning, GAIL trains a generator $G$ to learn the data distribution of the expert policy $\pi^*$ given by the recorded transition tuples while the discriminator tries to distinguish between proposed actions either from $\pi^*$ or the current policy $\pi^G$ of the generator.
