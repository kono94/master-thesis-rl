Although directly interacting RL methods show great performance in a variety of different fields such as robotics and autonomous control \cite[]{etemad2020using, patil2021deep, wang2018reinforcement}, healthcare \cite[]{tseng2017deep, yu2019incorporating},  communication and networking \cite[]{chinchali2018cellular, fadlullah2017state}  or games \cite[]{berner2019dota, mnih2013playing, wu2016training}, they have got some weaknesses. One of them being the formulation of the most important component in order to successfully solve a task - the reward function. Designing a suitable reward signal by hand is a challenging but crucial job. Given a complex environment with long decisions trajectories until the terminal state, chess, for example. If the reward is too sparse (feedback only at the very end of a game depending on the result), then the agent is likely to never explore a trajectory that results in winning the game and updating the state-action values accordingly. Contrary to that, a reward signal can be chosen so that it gives feedback in every state and almost explicitly tries to guide the agent. Such a reward function requires extensive task and domain knowledge, but is also very susceptible to the agent getting stuck in a local maximum. In the example of chess, this can be the assignment of a big positive reward when removing the opposing queen. Even though many players would agree, that getting rid of the enemy's queen is almost game winning, the agent might still not win a single game after that because it learns to just focus on removing the enemy's queen without having a strategy to actually win the game. Additionally, if the heuristic that determines the current board setting is defined by a human, the agent is bound to the human's view of playing the game, which is manifested in the reward function. 
\par
Another weakness of RL methods that sample and learn directly from the interaction with an environment is the pure amount of interactions needed for the purpose of discovering a good-performing policy. In the literature, most algorithms are tested and benchmarked on classic control tasks (\textit{CartPole}, \textit{MountainCar}, \textit{Pendulum}, etc.), more challenging tasks (\textit{HalfCheetah}, \textit{Walker2d}, \textit{Humanoid}, etc.) in a physics engine such as \textit{MuJoCo} \cite[]{todorov2012mujoco} or take place in the field of game theory with examples such as the \textit{Atari} games \cite[]{mnih2013playing}, \textit{Minecraft} \cite[]{johnson2016malmo}, \textit{Starcraft 2} \cite[]{vinyals2019grandmaster} or \textit{Dota 2} \cite[]{berner2019dota}. These environments as well as the ones used in the context of robotics or autonomous control are either digital by nature or simulations of the real-world, which can be sped up and run in parallel during training. On top of that, failing (or rather discovering bad states) during the training phase has no real-world consequences, like crashing a vehicle or hitting an obstacle with a remotely operated underwater vehicle (ROV), like the DLR-MI is using. Teaching an ROV to maneuver autonomously using interactive RL is near impossible without a high-quality simulation.
\par
But even if a simulation with appropriate input and output interfaces is present, the complexity of a given task can still demand huge amounts of computational power and millions of episodes. Good indicators for the complexity of a task are the dimensions of the state and action spaces, as well as the average amount of time steps per episode. We take the work from \cite{berner2019dota} as an extreme example as reference to our argument. They propose a system called \textit{OpenAI five}, which learns to play the popular and highly complex 5vs5 esports game \textit{Dota 2} on such a high level that it is able to defeat the world champions of the year 2018 (p.~1). Without going into detail about the game itself, the agent has to interact with an environment that produces episodes of around 20,000 time steps, an observation space of more than 16,000 states and a dynamic action space of 8,000 to 80,000 discrete actions \cite[p.~3]{berner2019dota}. To solve such a large problem in a way that it surpasses human performance is an impressive result, showing the potential of the reinforcement learning framework. However, the computing power and playtime required to discover such a policy far exceeds that of human, learning to play the game. The system consists of over 1,000 GPUs and ~51,000 CPUs \cite[p.~3]{berner2019dota} and was trained on 45,000 years of Dota self-play over 10 real-time months \cite[]{OpenAI_dota}.
Numbers like this are more reminiscent of brute forcing than the ability to learn like a human, though it is most certainly impossible that a brute force method will ever be able to play Dota 2.