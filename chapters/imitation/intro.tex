In the previous chapter, we dove deep into a popular algorithm that is part of the classic approach of online RL. Sampling experience in the form of transition tuples by direct interactions with an environment, those methods do not require a predefined dataset. This greatly separates them from other machine learning disciplines like supervised or unsupervised learning.
\par 
In this chapter, we introduce the reader to a deviant approach of receiving an intelligent agent that has a very close relationship to reinforcement learning - \textit{Imitation Learning}. Foremost, we will discuss the major challenges and downsides of classic interactive online RL, followed by an exotic side trip into the fields of psychology and nature to then interpret the vessel path prediction problem from a different perspective. Afterwards, we take a closer look at a method called \textit{Behavioral Cloning}, followed by an investigation of an approach that reconstructs a hidden reward function based on expert demonstrations, to then use classic online RL algorithms such as DDPG to learn the underlying policy. 